#!/usr/bin/env python3
"""
HR çŸ¥è­˜åº«æ™ºèƒ½æŸ¥è©¢ç³»çµ± - Streamlit éƒ¨ç½²ç‰ˆæœ¬

æ”¯æ´è³‡æ–™ä¾†æºï¼š
- å‹å‹•æ³•è¦ FAQï¼ˆå‹å‹•éƒ¨ã€å‹ä¿å±€ã€è·å®‰ç½²ï¼‰
- å‹å‹•æ¥­å‹™èªªæ˜
- ç¨…å‹™å•ç­”ï¼ˆç¶œåˆæ‰€å¾—ç¨…ï¼‰
- å¥ä¿æ¥­å‹™ï¼ˆæŠ•ä¿èˆ‡ä¿è²»ï¼‰
- æ³•è¦æ¢æ–‡ï¼ˆå…¨åœ‹æ³•è¦è³‡æ–™åº«ï¼‰
"""

import streamlit as st
import os
import re
import time
import json
from typing import List, Dict, Any
from pathlib import Path

# é é¢é…ç½®
st.set_page_config(
    page_title="äººè³‡æ³•è¦æ™ºèƒ½æŸ¥è©¢",
    page_icon="ğŸ‘·",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Store é…ç½®ï¼ˆ4 å€‹ Storeï¼‰- v2 ç‰ˆæœ¬
# 2025-11-29 æ›´æ–°ï¼šé‡å»º Store ä¿®å¾©é‡è¤‡æ–‡ä»¶å•é¡Œ
STORES = {
    # === å‹å‹•æ³•è¦ ===
    'labor_faq': {
        'name': 'krepo-labor-faq-v2',
        'store_id': 'fileSearchStores/krepolaborfaqv2-knbagjg20f9k',
        'display_name': 'å‹å‹•æ³•è¦FAQ',
        'icon': 'ğŸ‘·',
        'description': 'å‹å‹•éƒ¨ã€å‹ä¿å±€ã€è·å®‰ç½²å¸¸è¦‹å•ç­”',
        'count': 1691,
        'group': 'labor',
    },
    'labor_articles': {
        'name': 'krepo-labor-articles-v2',
        'store_id': 'fileSearchStores/krepolaborarticlesv2-c0q9a9rfsmok',
        'display_name': 'å‹å‹•èˆ‡å¥ä¿æ¥­å‹™',
        'icon': 'ğŸ“‹',
        'description': 'å‹å‹•éƒ¨æ¥­å‹™å°ˆå€ã€å‹ä¿å±€ä¿éšªæ¥­å‹™ã€å¥ä¿æŠ•ä¿èªªæ˜',
        'count': 626,
        'group': 'labor',
    },
    # === ç¨…å‹™ ===
    'tax_faq': {
        'name': 'krepo-tax-faq-v2',
        'store_id': 'fileSearchStores/krepotaxfaqv2-f7rnf4bjyo4f',
        'display_name': 'ç¨…å‹™å•ç­”',
        'icon': 'ğŸ’°',
        'description': 'ç¶œåˆæ‰€å¾—ç¨…å•èˆ‡ç­”',
        'count': 318,
        'group': 'tax',
    },
    # === æ³•è¦ ===
    'law_articles': {
        'name': 'krepo-law-articles-v2',
        'store_id': 'fileSearchStores/krepolawarticlesv2-s6rfdsug6uvx',
        'display_name': 'æ³•è¦æ¢æ–‡',
        'icon': 'ğŸ“–',
        'description': 'å…¨æ°‘å¥åº·ä¿éšªæ³•ç­‰æ³•è¦æ¢æ–‡',
        'count': 561,
        'group': 'law',
    },
}


def load_mappings():
    """
    è¼‰å…¥ gemini_mappings ç›®éŒ„ä¸‹çš„æ˜ å°„æª”æ¡ˆ

    æ ¼å¼: {doc_id: {gemini_file_id, store_id, source, title, date, original_url, uploaded_at}}
    """
    mappings_path = Path(__file__).parent.parent / "data" / "gemini_mappings"

    gemini_id_mapping = {}  # gemini_short_id -> doc_id
    file_mapping = {}       # doc_id -> info

    mapping_files = [
        "krepo-labor-faq-v2.json",
        "krepo-labor-articles-v2.json",
        "krepo-tax-faq-v2.json",
        "krepo-law-articles-v2.json",
    ]

    try:
        for filename in mapping_files:
            filepath = mappings_path / filename
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    raw_mapping = json.load(f)
                    for doc_id, info in raw_mapping.items():
                        # å»ºç«‹ gemini_id -> doc_id æ˜ å°„
                        gemini_file_id = info.get('gemini_file_id', '')
                        if gemini_file_id:
                            short_id = gemini_file_id.replace('files/', '')
                            gemini_id_mapping[short_id] = doc_id

                        # å»ºç«‹ doc_id -> info æ˜ å°„
                        file_mapping[doc_id] = {
                            'display_name': info.get('title', ''),
                            'date': info.get('date', ''),
                            'source': info.get('source', ''),
                            'store_id': info.get('store_id', ''),
                            'original_url': info.get('original_url', ''),
                        }

    except Exception as e:
        st.warning(f"è¼‰å…¥ mapping æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return gemini_id_mapping, file_mapping


# å…¨åŸŸ Mapping (è¼‰å…¥ä¸€æ¬¡)
GEMINI_ID_MAPPING, FILE_MAPPING = load_mappings()


def load_law_pcode_mapping() -> Dict[str, str]:
    """
    è¼‰å…¥æ³•è¦åç¨± â†’ pcode å°ç…§è¡¨

    ç”¨æ–¼å°‡æ³•è¦åç¨±è½‰æ›ç‚ºå…¨åœ‹æ³•è¦è³‡æ–™åº«é€£çµ
    """
    mapping_path = Path(__file__).parent.parent / "data" / "law_pcode_mapping.json"

    try:
        if mapping_path.exists():
            with open(mapping_path, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        st.warning(f"è¼‰å…¥æ³•è¦å°ç…§è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return {}


# å…¨åŸŸæ³•è¦å°ç…§è¡¨
LAW_PCODE_MAPPING = load_law_pcode_mapping()

# é å…ˆè¨ˆç®—æ’åºå¾Œçš„æ³•è¦åç¨±ï¼ˆé•·åº¦ç”±é•·åˆ°çŸ­ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…å•é¡Œï¼‰
SORTED_LAW_NAMES = sorted(LAW_PCODE_MAPPING.keys(), key=len, reverse=True)


def linkify_law_names(text: str) -> str:
    """
    å°‡æ–‡å­—ä¸­çš„æ³•è¦åç¨±è½‰æ›ç‚ºå¯é»æ“Šçš„é€£çµ

    æ”¯æ´æ ¼å¼ï¼š
    - ç´”æ³•è¦åç¨±ï¼šå‹å‹•åŸºæº–æ³• â†’ [å‹å‹•åŸºæº–æ³•](url)
    - æ³•è¦+æ¢æ–‡ï¼šå‹å‹•åŸºæº–æ³•ç¬¬38æ¢ â†’ [å‹å‹•åŸºæº–æ³•ç¬¬38æ¢](url)
    - åŒ…å«é …æ¬¾ï¼šå‹å‹•åŸºæº–æ³•ç¬¬11æ¢ç¬¬1é …ç¬¬5æ¬¾ â†’ [å‹å‹•åŸºæº–æ³•ç¬¬11æ¢ç¬¬1é …ç¬¬5æ¬¾](url)

    Args:
        text: åŸå§‹æ–‡å­—

    Returns:
        è½‰æ›å¾Œçš„æ–‡å­—ï¼ˆå« Markdown é€£çµï¼‰
    """
    if not text or not LAW_PCODE_MAPPING:
        return text

    # è¨˜éŒ„å·²æ›¿æ›çš„ä½ç½®ï¼Œé¿å…é‡è¤‡æ›¿æ›
    replaced_ranges = []

    def is_overlapping(start: int, end: int) -> bool:
        """æª¢æŸ¥æ˜¯å¦èˆ‡å·²æ›¿æ›çš„ç¯„åœé‡ç–Š"""
        for r_start, r_end in replaced_ranges:
            if start < r_end and end > r_start:
                return True
        return False

    # å»ºç«‹æ›¿æ›åˆ—è¡¨ï¼ˆä½ç½®, åŸå§‹æ–‡å­—, é€£çµæ–‡å­—ï¼‰
    replacements = []

    for law_name in SORTED_LAW_NAMES:
        pcode = LAW_PCODE_MAPPING[law_name]

        # åŒ¹é…æ³•è¦åç¨± + å¯é¸çš„æ¢æ–‡ç·¨è™Ÿï¼ˆç¬¬Xæ¢ã€ç¬¬Xæ¢ä¹‹Yã€ç¬¬Xæ¢ç¬¬Yé …ç¬¬Zæ¬¾ã€ä½†æ›¸ç­‰ï¼‰
        # æ”¯æ´å„ç¨®æ¨™é»ç¬¦è™ŸåŒ…åœçš„æ ¼å¼ï¼ˆGemini å¯èƒ½ä½¿ç”¨ã€Šã€‹ã€Œã€ã€ã€ã€ã€‘ç­‰ï¼‰
        # æ¢æ–‡ç·¨è™Ÿæ ¼å¼ï¼šç¬¬\d+æ¢(?:ä¹‹\d+)?(?:ç¬¬\d+é …)?(?:ç¬¬\d+æ¬¾)?(?:ä½†æ›¸)?
        pattern = r'[ã€Šã€Œã€ã€]?' + re.escape(law_name) + r'[ã€‹ã€ã€ã€‘]?(ç¬¬\d+æ¢(?:ä¹‹\d+)?(?:ç¬¬\d+é …)?(?:ç¬¬\d+æ¬¾)?(?:ä½†æ›¸)?)?'

        for match in re.finditer(pattern, text):
            start, end = match.span()

            # æª¢æŸ¥æ˜¯å¦å·²è¢«æ›¿æ›
            if is_overlapping(start, end):
                continue

            full_match = match.group(0)
            article_part = match.group(1)  # å¯èƒ½æ˜¯ None

            # å»ºæ§‹ URL
            if article_part:
                # æå–æ¢æ–‡ç·¨è™Ÿï¼ˆé˜¿æ‹‰ä¼¯æ•¸å­—ï¼‰
                article_match = re.search(r'ç¬¬(\d+)æ¢', article_part)
                if article_match:
                    article_num = article_match.group(1)
                    # å–®ä¸€æ¢æ–‡é é¢
                    url = f"https://law.moj.gov.tw/LawClass/LawSingle.aspx?pcode={pcode}&flno={article_num}"
                else:
                    # æ•´éƒ¨æ³•è¦é é¢
                    url = f"https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode={pcode}"
            else:
                # æ•´éƒ¨æ³•è¦é é¢
                url = f"https://law.moj.gov.tw/LawClass/LawAll.aspx?pcode={pcode}"

            # å»ºç«‹ Markdown é€£çµ
            link = f"[{full_match}]({url})"

            replacements.append((start, end, full_match, link))
            replaced_ranges.append((start, end))

    # æŒ‰ä½ç½®å€’åºæ’åˆ—ï¼Œå¾å¾Œå¾€å‰æ›¿æ›ï¼ˆé¿å…ä½ç½®åç§»ï¼‰
    replacements.sort(key=lambda x: x[0], reverse=True)

    result = text
    for start, end, original, link in replacements:
        result = result[:start] + link + result[end:]

    return result


def linkify_law_section(text: str) -> str:
    """
    åªåœ¨ã€Œç›¸é—œæ³•è¦ã€å€å¡Šå°‡æ³•è¦åç¨±è½‰æ›ç‚ºé€£çµ

    æ‰¾åˆ°ã€Œç›¸é—œæ³•è¦:ã€æˆ–ã€Œç›¸é—œæ³•è¦ï¼šã€å¾Œçš„å…§å®¹ï¼Œåªå°è©²éƒ¨åˆ†å¥—ç”¨é€£çµè½‰æ›ã€‚
    å…¶ä»–å…§æ–‡ä¿æŒåŸæ¨£ã€‚

    Args:
        text: åŸå§‹ç­”æ¡ˆæ–‡å­—

    Returns:
        è½‰æ›å¾Œçš„æ–‡å­—
    """
    if not text:
        return text

    # å°‹æ‰¾ã€Œç›¸é—œæ³•è¦ã€å€å¡Šçš„å„ç¨®å¯èƒ½æ ¼å¼
    patterns = [
        r'(ç›¸é—œæ³•è¦[ï¼š:]\s*)',      # ç›¸é—œæ³•è¦: æˆ– ç›¸é—œæ³•è¦ï¼š
        r'(ç›¸é—œæ³•è¦å¼•ç”¨[ï¼š:]\s*)',  # ç›¸é—œæ³•è¦å¼•ç”¨:
        r'(å¼•ç”¨æ³•è¦[ï¼š:]\s*)',      # å¼•ç”¨æ³•è¦:
        r'(æ³•è¦ä¾æ“š[ï¼š:]\s*)',      # æ³•è¦ä¾æ“š:
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            # æ‰¾åˆ°æ¨™é¡Œä½ç½®
            header_end = match.end()

            # è·³éæ¨™é¡Œå¾Œçš„ç©ºç™½è¡Œï¼ˆå¯èƒ½æœ‰ \n\n æˆ–å¤šå€‹ç©ºè¡Œï¼‰
            remaining_after_header = text[header_end:]
            skip_match = re.match(r'^[\s\n]*', remaining_after_header)
            if skip_match:
                content_start = header_end + skip_match.end()
            else:
                content_start = header_end

            # å¾å…§å®¹é–‹å§‹è™•æ‰¾çµå°¾
            remaining = text[content_start:]

            # æ‰¾ä¸‹ä¸€å€‹æ˜é¡¯çš„æ®µè½åˆ†éš”ï¼ˆå…©å€‹ä»¥ä¸Šçš„æ›è¡Œï¼Œæˆ–ç‰¹æ®Šæ¨™è¨˜é–‹é ­ï¼‰
            # æ’é™¤æ³•è¦åˆ—è¡¨ä¸­çš„å–®ä¸€æ›è¡Œ
            end_match = re.search(r'\n\n\n|\n\n(?=[^ã€Šã€Œã€ã€\n])', remaining)
            if end_match:
                end_pos = content_start + end_match.start()
            else:
                # åˆ°æ–‡ä»¶çµå°¾
                end_pos = len(text)

            law_section = text[content_start:end_pos]
            # åªå°æ³•è¦å€å¡Šå¥—ç”¨é€£çµ
            linked_section = linkify_law_names(law_section)
            return text[:content_start] + linked_section + text[end_pos:]

    # æ²’æ‰¾åˆ°ç›¸é—œæ³•è¦å€å¡Šï¼Œè¿”å›åŸæ–‡
    return text


def resolve_source_display_name(raw_id: str) -> tuple:
    """
    å°‡ Gemini å›å‚³çš„ file ID è§£æç‚ºå¯è®€çš„é¡¯ç¤ºåç¨±

    å›å‚³: (display_name, source_type, date, original_url)
    """
    # å…ˆæª¢æŸ¥ raw_id æ˜¯å¦ç›´æ¥æ˜¯ doc_id
    if raw_id in FILE_MAPPING:
        doc_id = raw_id
    else:
        # å˜—è©¦å¾ gemini_id -> doc_id æ˜ å°„æŸ¥è©¢
        doc_id = GEMINI_ID_MAPPING.get(raw_id, '')

    if doc_id and doc_id in FILE_MAPPING:
        info = FILE_MAPPING[doc_id]
        display_name = info.get('display_name', '')
        date = info.get('date', 'æœªçŸ¥æ—¥æœŸ')
        source = info.get('source', '')
        original_url = info.get('original_url', '')

        # åˆ¤æ–·ä¾†æºé¡å‹å’Œåœ–æ¨™
        if 'mol_faq' in source:
            source_type = "å‹å‹•éƒ¨FAQ"
            icon = "ğŸ‘·"
        elif 'bli_faq' in source:
            source_type = "å‹ä¿å±€FAQ"
            icon = "ğŸ¢"
        elif 'osha_faq' in source:
            source_type = "è·å®‰ç½²FAQ"
            icon = "âš ï¸"
        elif 'mol_business' in source:
            source_type = "å‹å‹•éƒ¨æ¥­å‹™"
            icon = "ğŸ“‹"
        elif 'bli_insurance' in source:
            source_type = "å‹ä¿å±€æ¥­å‹™"
            icon = "ğŸ“„"
        elif 'individual_income_tax' in source:
            source_type = "ç¨…å‹™å•ç­”"
            icon = "ğŸ’°"
        elif 'insurance_premium' in source:
            source_type = "å¥ä¿æ¥­å‹™"
            icon = "ğŸ¥"
        elif 'law_articles' in source:
            source_type = "æ³•è¦æ¢æ–‡"
            icon = "ğŸ“–"
        else:
            source_type = "æœªçŸ¥"
            icon = "ğŸ“„"

        # æ ¼å¼åŒ–é¡¯ç¤ºåç¨±
        if display_name:
            # æˆªæ–·éé•·çš„æ¨™é¡Œ
            short_title = display_name[:40] + "..." if len(display_name) > 40 else display_name
            return f"{icon} {short_title}", source_type, date, original_url

        return f"{icon} {source_type}_{date}", source_type, date, original_url

    # å¦‚æœ mapping æ‰¾ä¸åˆ°
    return f"ğŸ“„ {raw_id[:30]}...", "æœªçŸ¥", "æœªçŸ¥æ—¥æœŸ", ""


# ç¯„ä¾‹å•é¡Œ
EXAMPLE_QUESTIONS = [
    "å‹å·¥é€€ä¼‘é‡‘æç¹³ç‡æ˜¯å¤šå°‘ï¼Ÿ",
    "è³‡é£è²»å¦‚ä½•è¨ˆç®—ï¼Ÿ",
    "è‚²å¬°ç•™è·åœè–ªæœŸé–“å¥ä¿æ€éº¼è™•ç†ï¼Ÿ",
    "åŠ ç­è²»è¦å¦‚ä½•è¨ˆç®—ï¼Ÿ",
    "è·ç½è£œå„Ÿæœ‰å“ªäº›é …ç›®ï¼Ÿ",
    "æ‰£ç¹³æ†‘å–®ä»€éº¼æ™‚å€™è¦ç”³å ±ï¼Ÿ",
]


def get_system_prompt(selected_stores: List[str]) -> str:
    """
    æ ¹æ“šé¸å–çš„ Store çµ„åˆç”¢ç”Ÿç³»çµ±æç¤º
    """
    base_prompt = """ä½ æ˜¯å°ˆæ¥­çš„ HR æ³•è¦é¡§å•ï¼Œç†Ÿæ‚‰å°ç£å‹å‹•æ³•è¦ã€ç¨…å‹™è¦å®šèˆ‡å¥ä¿åˆ¶åº¦ã€‚

è«‹æ ¹æ“šåƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚

å›ç­”æ™‚å¿…é ˆï¼š
1. æ˜ç¢ºå¼•ç”¨ä¾†æºæ–‡ä»¶
2. å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦èªªæ˜
3. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
4. ä¿æŒå°ˆæ¥­ã€å®¢è§€çš„æ…‹åº¦
"""

    # æ ¹æ“šé¸å–çš„ Store åŠ å…¥ç‰¹å®šæŒ‡å¼•
    specific_guidelines = []

    if 'labor_faq' in selected_stores:
        specific_guidelines.append("""
ã€å‹å‹•æ³•è¦FAQæŒ‡å¼•ã€‘
- ç›´æ¥å¼•ç”¨å¸¸è¦‹å•ç­”å…§å®¹
- èªªæ˜ç›¸é—œæ³•è¦ä¾æ“š
- åˆ—å‡ºé©ç”¨å°è±¡å’Œæ¢ä»¶""")

    if 'labor_articles' in selected_stores:
        specific_guidelines.append("""
ã€å‹å‹•æ¥­å‹™èªªæ˜æŒ‡å¼•ã€‘
- è§£é‡‹æ¥­å‹™æµç¨‹å’Œè¦å®š
- èªªæ˜ç”³è«‹æ–¹å¼å’Œæ¢ä»¶
- åˆ—å‡ºæ‰€éœ€æ–‡ä»¶""")

    if 'tax_faq' in selected_stores:
        specific_guidelines.append("""
ã€ç¨…å‹™å•ç­”æŒ‡å¼•ã€‘
- è§£é‡‹ç¨…å‹™è¦å®š
- èªªæ˜ç”³å ±æ–¹å¼å’ŒæœŸé™
- åˆ—å‡ºå…ç¨…æˆ–æ¸›ç¨…æ¢ä»¶""")

    if 'nhi_insurance' in selected_stores:
        specific_guidelines.append("""
ã€å¥ä¿æ¥­å‹™æŒ‡å¼•ã€‘
- èªªæ˜æŠ•ä¿è³‡æ ¼å’Œæ¢ä»¶
- è§£é‡‹ä¿è²»è¨ˆç®—æ–¹å¼
- èªªæ˜ç¹³è²»è¦å®š""")

    if 'law_articles' in selected_stores:
        specific_guidelines.append("""
ã€æ³•è¦æ¢æ–‡æŒ‡å¼•ã€‘
- å¼•ç”¨å®Œæ•´çš„æ³•æ¢å…§å®¹
- èªªæ˜æ³•æ¢çš„é©ç”¨æƒ…æ³""")

    if specific_guidelines:
        return base_prompt + "\n" + "\n".join(specific_guidelines)
    return base_prompt


def query_gemini(
    question: str,
    selected_stores: List[str],
    api_key: str,
    model: str = 'gemini-2.5-flash'
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ Gemini File Search åŸ·è¡ŒæŸ¥è©¢
    """
    from google import genai
    from google.genai import types

    client = genai.Client(api_key=api_key)

    # å–å¾—é¸å–çš„ Store IDs
    store_ids = [STORES[s]['store_id'] for s in selected_stores if s in STORES and STORES[s]['store_id']]

    if not store_ids:
        return {
            'answer': 'è«‹è‡³å°‘é¸æ“‡ä¸€å€‹è³‡æ–™ä¾†æºï¼ˆæˆ– Store å°šæœªè¨­å®šï¼‰',
            'sources': [],
            'error': True,
            'empty_answer': False
        }

    # å–å¾—ç³»çµ±æç¤º
    system_prompt = get_system_prompt(selected_stores)

    start_time = time.time()

    try:
        response = client.models.generate_content(
            model=model,
            contents=question,
            config=types.GenerateContentConfig(
                tools=[
                    types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=store_ids
                        )
                    )
                ],
                temperature=0.1,
                max_output_tokens=8000,
                system_instruction=system_prompt
            )
        )

        latency = time.time() - start_time

        # æå–ç­”æ¡ˆ
        answer = None
        empty_answer = True
        if hasattr(response, 'text') and response.text:
            answer = response.text
            empty_answer = False

        # æå–ä¾†æº
        sources = extract_sources(response)

        return {
            'answer': answer,
            'sources': sources,
            'latency': latency,
            'error': False,
            'empty_answer': empty_answer,
        }

    except Exception as e:
        return {
            'answer': f'æŸ¥è©¢å¤±æ•—: {str(e)}',
            'sources': [],
            'error': True,
            'empty_answer': False
        }


def extract_sources(response) -> List[Dict[str, Any]]:
    """
    å¾ Gemini å›æ‡‰ä¸­æå–ä¾†æºï¼ˆå·²å»é‡ï¼‰
    """
    sources = []
    seen_ids = set()

    try:
        if hasattr(response, 'candidates') and len(response.candidates) > 0:
            candidate = response.candidates[0]

            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                metadata = candidate.grounding_metadata

                if hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                    for chunk in metadata.grounding_chunks:
                        if hasattr(chunk, 'retrieved_context'):
                            context = chunk.retrieved_context

                            # æå–åŸå§‹æª”å/ID
                            raw_id = ""
                            if hasattr(context, 'title') and context.title:
                                raw_id = context.title
                            elif hasattr(context, 'uri') and context.uri:
                                raw_id = context.uri.split('/')[-1]

                            # å»é‡
                            if raw_id in seen_ids:
                                continue
                            seen_ids.add(raw_id)

                            # ä½¿ç”¨ mapping è§£æé¡¯ç¤ºåç¨±
                            display_name, source_type, date, original_url = resolve_source_display_name(raw_id)

                            snippet = ""
                            if hasattr(context, 'text') and context.text:
                                snippet = context.text[:500]

                            score = 1.0
                            if hasattr(chunk, 'score'):
                                score = float(chunk.score)

                            sources.append({
                                'filename': display_name,
                                'raw_id': raw_id,
                                'source_type': source_type,
                                'date': date,
                                'snippet': snippet,
                                'score': score,
                                'original_url': original_url,
                            })

    except Exception as e:
        st.warning(f"æå–ä¾†æºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    return sources


def render_sidebar():
    """æ¸²æŸ“å´é‚Šæ¬„"""
    with st.sidebar:
        st.header("ğŸ“Š è³‡æ–™ä¾†æº")

        # é¸æ“‡å…¨éƒ¨ / å–æ¶ˆé¸æ“‡ æŒ‰éˆ•
        col1, col2 = st.columns(2)
        with col1:
            if st.button("âœ… å…¨é¸", use_container_width=True):
                for key in STORES.keys():
                    st.session_state[f"store_{key}"] = True
                st.rerun()
        with col2:
            if st.button("âŒ å–æ¶ˆ", use_container_width=True):
                for key in STORES.keys():
                    st.session_state[f"store_{key}"] = False
                st.rerun()

        st.markdown("---")

        selected_stores = []

        # === å‹å‹•æ³•è¦ ===
        st.caption("ğŸ‘· å‹å‹•æ³•è¦")
        for key, store in STORES.items():
            if store.get('group') != 'labor':
                continue
            default_value = (key == 'labor_faq') if f"store_{key}" not in st.session_state else None
            checked = st.checkbox(
                f"{store['icon']} {store['display_name']}",
                value=default_value if default_value is not None else None,
                key=f"store_{key}",
                help=store['description']
            )
            if checked:
                selected_stores.append(key)

        st.markdown("---")

        # === ç¨…å‹™ ===
        st.caption("ğŸ’° ç¨…å‹™")
        for key, store in STORES.items():
            if store.get('group') != 'tax':
                continue
            checked = st.checkbox(
                f"{store['icon']} {store['display_name']}",
                key=f"store_{key}",
                help=store['description']
            )
            if checked:
                selected_stores.append(key)

        st.markdown("---")

        # === æ³•è¦ ===
        st.caption("ğŸ“– æ³•è¦æ¢æ–‡")
        for key, store in STORES.items():
            if store.get('group') != 'law':
                continue
            checked = st.checkbox(
                f"{store['icon']} {store['display_name']}",
                key=f"store_{key}",
                help=store['description']
            )
            if checked:
                selected_stores.append(key)

        st.markdown("---")

        # æª¢æŸ¥æ˜¯å¦é¸å–è³‡æ–™ä¾†æº
        if not selected_stores:
            st.warning("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹è³‡æ–™ä¾†æº")

        st.markdown("---")

        # ä½¿ç”¨èªªæ˜
        st.header("ğŸ“– ä½¿ç”¨èªªæ˜")
        st.markdown("""
1. **é¸æ“‡è³‡æ–™ä¾†æº**ï¼šå‹¾é¸è¦æŸ¥è©¢çš„çŸ¥è­˜åº«
2. **è¼¸å…¥å•é¡Œ**ï¼šåœ¨ä¸»ç•«é¢è¼¸å…¥æ‚¨çš„å•é¡Œ
3. **é»æ“ŠæŸ¥è©¢**ï¼šAI æœƒå¾é¸å–çš„çŸ¥è­˜åº«ä¸­æœå°‹ç›¸é—œè³‡æ–™ä¸¦å›ç­”
4. **æŸ¥çœ‹ä¾†æº**ï¼šå±•é–‹åƒè€ƒä¾†æºå¯æŸ¥çœ‹åŸå§‹å…§å®¹
""")

    return selected_stores


def main():
    """ä¸»ç¨‹å¼"""
    # å–å¾— API Key
    api_key = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")

    if not api_key:
        st.error("è«‹è¨­å®š GEMINI_API_KEY")
        st.stop()

    # æ¸²æŸ“å´é‚Šæ¬„
    selected_stores = render_sidebar()

    # ä¸»æ¨™é¡Œ
    st.title("ğŸ‘· äººè³‡æ³•è¦æ™ºèƒ½æŸ¥è©¢")
    st.caption("âš ï¸ æœ¬ç³»çµ±ç‚ºå±•ç¤ºç”¨ï¼Œå¦‚é‡ç•«é¢ç„¡åæ‡‰ï¼Œè«‹é‡æ–°æ•´ç†é é¢")

    # å•é¡Œè¼¸å…¥
    if 'current_question' not in st.session_state:
        st.session_state.current_question = ""

    question = st.text_area(
        "è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š",
        value=st.session_state.current_question,
        placeholder="ä¾‹å¦‚ï¼šè³‡é£è²»å¦‚ä½•è¨ˆç®—ï¼Ÿ",
        height=100
    )

    # æŒ‰éˆ•åˆ—
    col1, col2, col3 = st.columns([1, 1, 4])

    with col1:
        submit_button = st.button("ğŸ” æŸ¥è©¢", type="primary", use_container_width=True)

    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤", use_container_width=True):
            st.session_state.current_question = ""
            st.rerun()

    # è™•ç†æŸ¥è©¢
    if submit_button and question:
        if not selected_stores:
            st.error("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹è³‡æ–™ä¾†æº")
        else:
            with st.spinner("ğŸ” AI æŸ¥è©¢ä¸­..."):
                result = query_gemini(
                    question,
                    selected_stores,
                    api_key
                )

            if result['error']:
                st.error(result['answer'])
            elif result['empty_answer'] or not result['sources']:
                st.warning("âš ï¸ æŸ¥è©¢æœªèƒ½å–å¾—æœ‰æ•ˆå›ç­”")
                st.info("è«‹å˜—è©¦æ›å€‹æ–¹å¼æè¿°æ‚¨çš„å•é¡Œã€‚")
            else:
                # é¡¯ç¤ºçµæœ
                st.success("âœ… æŸ¥è©¢å®Œæˆ")

                stores_text = ", ".join([STORES[s]['display_name'] for s in selected_stores])
                st.caption(f"â±ï¸ å›æ‡‰æ™‚é–“: {result['latency']:.2f} ç§’ã€€ï½œã€€ğŸ“š ä¾†æºæ•¸é‡: {len(result['sources'])} ç­†ã€€ï½œã€€ğŸ“‚ æŸ¥è©¢ç¯„åœ: {stores_text}")

                st.markdown("---")

                # ç­”æ¡ˆ
                st.subheader("ğŸ“ ç­”æ¡ˆ")
                # å…¨å…§å®¹æ¯”å°æ³•è¦åç¨±ï¼Œç”¢ç”Ÿé€£çµï¼ˆä¸é™æ–¼ç‰¹å®šå€å¡Šï¼‰
                answer_with_links = linkify_law_names(result['answer'])
                st.markdown(answer_with_links)

                st.markdown("---")

                # ä¾†æºï¼ˆå»é‡è¤‡ï¼‰
                if result['sources']:
                    # ä»¥ filename å»é‡è¤‡
                    seen_filenames = set()
                    unique_sources = []
                    for source in result['sources']:
                        if source['filename'] not in seen_filenames:
                            seen_filenames.add(source['filename'])
                            unique_sources.append(source)

                    st.subheader(f"ğŸ“š åƒè€ƒä¾†æº ({len(unique_sources)} ç­†)")

                    # æŒ‰é¡å‹åˆ†çµ„
                    source_groups = {}
                    for source in unique_sources:
                        stype = source.get('source_type', 'æœªçŸ¥')
                        if stype not in source_groups:
                            source_groups[stype] = []
                        source_groups[stype].append(source)

                    # é¡¯ç¤ºå„çµ„
                    for stype, sources_list in source_groups.items():
                        st.caption(f"{stype} ({len(sources_list)} ç­†)")
                        for source in sources_list:
                            with st.expander(f"{source['filename']}", expanded=False):
                                st.markdown(f"**ç›¸é—œå…§å®¹ï¼š**")
                                st.markdown(f"> {source['snippet'][:300]}...")

                                if source.get('original_url'):
                                    st.markdown(f"[ğŸ”— æŸ¥çœ‹åŸå§‹ç¶²é ]({source['original_url']})")

    # ç¯„ä¾‹å•é¡Œ
    if not question:
        st.markdown("---")
        st.subheader("ğŸ’¡ ç¯„ä¾‹å•é¡Œ")

        cols = st.columns(2)
        for idx, eq in enumerate(EXAMPLE_QUESTIONS):
            col = cols[idx % 2]
            with col:
                if st.button(f"ğŸ“Œ {eq}", key=f"example_{idx}", use_container_width=True):
                    st.session_state.current_question = eq
                    st.rerun()

    # é å°¾
    st.divider()
    st.caption("è³‡æ–™ä¾†æºï¼šæ„è—è³‡è¨Šå‹å‹•çŸ¥è­˜åº«")


if __name__ == "__main__":
    main()
